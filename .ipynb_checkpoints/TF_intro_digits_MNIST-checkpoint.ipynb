{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will be training a model to classify MNIST (handwritten digits) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#y is the label vector, and x is the feature vector (we divide by 255 to scale down each feature?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model is a bit of a black box to me - I need to understand how to choose all of its components\n",
    "tf.keras.backend.set_floatx('float64') #this prevents errors later\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'), # how was the # of nodes chosen, why relu activation function\n",
    "  tf.keras.layers.Dropout(0.2), # what does this do\n",
    "  tf.keras.layers.Dense(10) # has as many nodes as there are classes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36294173, -0.0435469 ,  0.44666042,  0.52221191,  0.12482971,\n",
       "        -0.37519708, -0.13138243,  0.35013782, -0.15467523,  0.42845263]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we havn't trained our model yet, so these predictions will be random\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions # these predictions are log-odds (they need to be scaled into a probabilty distribution using softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06096814, 0.08391013, 0.13699626, 0.14774756, 0.09929775,\n",
       "        0.06022552, 0.07685425, 0.12439115, 0.07508479, 0.13452444]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since we are computing probabilities for each of the inputs being one of 10 different digits (but our model is untrained),\n",
    "#the probabilty for each class will be close to 1/10 (which it is)\n",
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the loss function which we use to optimize\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "#here the loss function has been passed in - how do we choose a good loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 0.2952 - accuracy: 0.9159\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 0.1389 - accuracy: 0.9587\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.1080 - accuracy: 0.9671\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0862 - accuracy: 0.9735\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0738 - accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x229ee74ec48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is where the loss function will be minimized and parameters will be better fit\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.0722 - accuracy: 0.9775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07216692353106384, 0.9775]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are not doing a validation step here, so we can straight away test accuracy on test set:\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "#accuracy is close to 98%, meaning our model is pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how we get all the probablities for each class:\n",
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float64, numpy=\n",
       "array([[8.93567303e-08, 2.68015582e-11, 1.30612728e-06, 9.35149404e-05,\n",
       "        9.74078301e-12, 2.92919887e-06, 1.74252916e-13, 9.99897070e-01,\n",
       "        2.68654307e-07, 4.82152628e-06],\n",
       "       [7.32084134e-08, 5.11496860e-05, 9.99884982e-01, 6.31584905e-05,\n",
       "        1.08465926e-15, 2.18668958e-08, 2.84809085e-09, 5.77169659e-11,\n",
       "        6.11856344e-07, 1.87867403e-12],\n",
       "       [1.18913064e-07, 9.97652463e-01, 2.29064344e-05, 7.49025582e-06,\n",
       "        1.24060558e-05, 1.18153912e-06, 1.77678089e-05, 2.16934475e-03,\n",
       "        1.13405562e-04, 2.91612071e-06],\n",
       "       [9.99882110e-01, 7.33200234e-10, 2.19375454e-05, 1.76583274e-07,\n",
       "        2.00471479e-09, 1.32350811e-06, 9.63259743e-06, 3.72278052e-05,\n",
       "        1.89843062e-08, 4.75701574e-05],\n",
       "       [2.08076411e-05, 1.98545971e-09, 1.20092929e-05, 2.00912800e-07,\n",
       "        9.87442661e-01, 2.12319115e-07, 5.95662026e-06, 1.58254578e-04,\n",
       "        8.35135331e-07, 1.23590601e-02]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_model(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]\n",
    "\n",
    "#if we look at the probabilities above for the x_test[0] case, we can see that the probability for \"7\" was the highest, \n",
    "#and we can see that the label for that specific case (labeled in y_test[0]) is indeed 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
